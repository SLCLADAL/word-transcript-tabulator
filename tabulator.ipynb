{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f34495de-b0f6-490a-bc86-00359b542f96",
   "metadata": {},
   "source": [
    "# Word Transcript Tabulator Recipe\n",
    "\n",
    "This notebook is a computational recipe to help you to take a collection of transcripts in word documents (_docx_ only) and turn them into a structured tabular format suitable for working with computational text analytics.\n",
    "\n",
    "As a side-effect of this, it will also help you check for and identify some inconsistencies in your transcripts (for example, inconsistent speaker identifiers).\n",
    "\n",
    "Note that this approach will remove all formatting from your text: if you have information in *bold*, _italics_, or elements highlighted in different colours that information will be lost in this process.\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "1. Upload your transcripts in `.docx` format to the transcripts folder.\n",
    "2. Run the script to produce an initial file for examination and manual changes.\n",
    "3. Check that the headers have been correctly been identified. (How???)\n",
    "4. Use the produced metadata to identify and fix inconsistencies in the transcripts *in word* - reupload any changed versions and try again.\n",
    "5. Fill in the metadata for speaker_codes and speakers. (Match codes in transcripts to speaker_ids, give details known about speakers).\n",
    "\n",
    "(This is going to need screenshots...)\n",
    "\n",
    "## Data Model\n",
    "\n",
    "## Libraries We'll Use\n",
    "\n",
    "## TODO: how to do this iteratively?\n",
    "\n",
    "## USECASE: while you're in data collection stage!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878e607c-dee7-4120-858d-0e75ac5cc6db",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "1. Upload your transcripts in `.docx` format in the transcripts folder on the left. Nested files are currently not supported.\n",
    "2. Run the code below.\n",
    "\n",
    "\n",
    "## TODO: Breakdown the functions and explain them one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba96007-1784-41aa-a2ee-c21ee15dd7bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "library(officer)\n",
    "library(writexl)\n",
    "library(dplyr)\n",
    "library(stringr)\n",
    "\n",
    "# Define some helper functions: these are how we extract individual units of data from your transcripts\n",
    "\n",
    "# Try to guess where a transcript might start after any header information - this can be fixed in the next step.\n",
    "guess_header_rows <- function(doc_summary){\n",
    "    # Go through all of the lines 1 by 1 until they match one of the potential ends of the header block\n",
    "    for (i in 1:nrow(doc_summary)){\n",
    "        row = doc_summary[i,]\n",
    "        # Header might end with a first blank line\n",
    "        if (trimws(row$text) == '') {\n",
    "            break\n",
    "        }\n",
    "        # Header ends with first list paragraph style (ie, if you use numbered lists to generate turn numbers)\n",
    "        else if (!is.na(row$style_name) & row$style_name == 'List Paragraph'){\n",
    "            i <- i - 1\n",
    "            break\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # If we can't find an end of header match, mark nothing as the header.\n",
    "    if (i == nrow(doc_summary)){\n",
    "        i <- 1\n",
    "    }\n",
    "\n",
    "    doc_summary$header = FALSE\n",
    "    doc_summary$header[1:i] = TRUE\n",
    "\n",
    "    return(doc_summary)\n",
    "}\n",
    "\n",
    "\n",
    "# TODO: extract any timecodes\n",
    "\n",
    "# Do the initial extraction from a particular file\n",
    "prepare_transcripts <- function(transcript_file_path){\n",
    "    transcript_df <- officer::docx_summary(officer::read_docx(transcript_file_path))\n",
    "    \n",
    "    # Attach the filepath as a column so we can trace this back\n",
    "    transcript_df$source_file <- basename(transcript_file_path)\n",
    "    \n",
    "    # Keep a copy of the original text so we can always compare the processing\n",
    "    transcript_df$source_text <- transcript_df$text\n",
    "    \n",
    "    # Extract the speaker codes via regex - match up to, but don't include the first colon.\n",
    "    speaker_match = '^[[:alpha:]]+?(?=:)'\n",
    "    transcript_df$speaker_code <- stringr::str_extract(transcript_df$source_text, speaker_match)\n",
    "    \n",
    "    # And replace the original text with the speaker code removed\n",
    "    transcript_df$text <- NULL\n",
    "    transcript_df$text <- str_replace(transcript_df$source_text, '^[[:alpha:]]+?:', \"\")\n",
    "    \n",
    "    transcript_df <- guess_header_rows(transcript_df)\n",
    "    \n",
    "    return(transcript_df)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad2c595-8d47-4945-a1f8-c85d7e4289a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now let's actually load your transcripts\n",
    "\n",
    "# List all of the docx files in the transcripts folder\n",
    "transcript_files = list.files(\"transcripts\", full.names=TRUE, pattern=\"*.docx\")\n",
    "\n",
    "paste0(\"Loading: \", length(transcript_files), \" transcripts:\") \n",
    "\n",
    "# TODO: figure out how to handle warnings without scaring people and ignoring them...\n",
    "# Load each transcript, extract the paragraphs of the document as rows in a dataframe, using the officer package\n",
    "loaded_docs <- lapply(transcript_files, prepare_transcripts)\n",
    "\n",
    "# Combine the transcripts together\n",
    "combined_transcripts <- bind_rows(loaded_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287f7acd-728c-4f07-84db-d2a8e4bc2e2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now prepare three views of this dataset:\n",
    "\n",
    "# Summary of the files and number of turns\n",
    "file_turns <- combined_transcripts %>% \n",
    "    filter(header==FALSE) %>% \n",
    "    group_by(source_file) %>% \n",
    "    count(name=\"turns\")\n",
    "\n",
    "# Info about the speaker-codes in each transcript\n",
    "speaker_code_summary <- combined_transcripts %>% \n",
    "    filter(header==FALSE) %>%\n",
    "    group_by(source_file, speaker_code) %>%\n",
    "    summarise(n_turns=n(), first_turn=min(doc_index), last_turn=max(doc_index))\n",
    "    \n",
    "\n",
    "# An (initially empty) table for speaker information\n",
    "\n",
    "sheets <- list(\n",
    "    transcripts = file_turns, \n",
    "    turns = combined_transcripts, \n",
    "    speaker_codes = speaker_code_summary\n",
    ")\n",
    "\n",
    "write_xlsx(\n",
    "    sheets,\n",
    "    path='combined_transcripts.xlsx'\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684aaa5e-8e4f-4ebd-b784-6237f6550cc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_transcripts %>% \n",
    "    filter(header==FALSE) %>%\n",
    "    group_by(source_file, speaker_code) %>%\n",
    "    summarise(n_turns=n(), first_turn=min(doc_index), last_turn=max(doc_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290a04b8-4165-440c-a592-f3f50265fdb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
